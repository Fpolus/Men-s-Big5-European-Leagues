{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618c379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from splinter import Browser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f8603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               team      link\n",
      "0   Manchester City  b8fd03ef\n",
      "1    Manchester Utd  19538871\n",
      "2         Tottenham  361ca564\n",
      "3         Liverpool  822bd0ba\n",
      "4           Chelsea  cff3d9bb\n",
      "5           Arsenal  18bb7c10\n",
      "6           Burnley  943e8050\n",
      "7           Everton  d3fd31cc\n",
      "8    Leicester City  a2d435b3\n",
      "9     Newcastle Utd  b2b47a98\n",
      "10   Crystal Palace  47c64c55\n",
      "11      Bournemouth  4ba7cbea\n",
      "12         West Ham  7c21e445\n",
      "13          Watford  2abfe087\n",
      "14         Brighton  d07537b9\n",
      "15     Huddersfield  f5922ca5\n",
      "16      Southampton  33c895d4\n",
      "17     Swansea City  fb10988f\n",
      "18       Stoke City  17892952\n",
      "19        West Brom  60c6b05f\n",
      "20           Wolves  8cec06e1\n",
      "21     Cardiff City  75fae011\n",
      "22           Fulham  fd962109\n",
      "23    Sheffield Utd  1df6b87e\n",
      "24      Aston Villa  8602292d\n",
      "25     Norwich City  1c781004\n",
      "26     Leeds United  5bfb9659\n",
      "27        Brentford  cd051869\n",
      "28  Nott'ham Forest  e4a775cb\n",
      "29       Luton Town  e297cd13\n"
     ]
    }
   ],
   "source": [
    "teams_df = pd.read_csv('../Resources/all_premier_league_teams.csv')\n",
    "teams_df.head()\n",
    "\n",
    "# Extracting the ID from the 'link' column in the DataFrame\n",
    "teams_df['link'] = teams_df['link'].apply(lambda x: x.split('/')[-3])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(teams_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc4799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the starting year (YYYY format): 2017\n",
      "Enter the ending year (type in same year for single year analysis): 2023\n",
      "Team: Manchester City, URLs: https://fbref.com/en/squads/b8fd03ef/2017-2018/Manchester-City-Stats\n"
     ]
    }
   ],
   "source": [
    "# Prompt user for the common year range\n",
    "year_i = input('Enter the starting year (YYYY format): ')\n",
    "year_f = input('Enter the ending year (type in same year for single year analysis): ')\n",
    "\n",
    "# Convert user inputs to integers\n",
    "year_i = int(year_i)\n",
    "year_f = int(year_f)\n",
    "\n",
    "# Create a Chrome browser instance\n",
    "browser = Browser('chrome')\n",
    "\n",
    "# Iterate through each team in the DataFrame using the common year range\n",
    "for index, row in teams_df.iterrows():\n",
    "    team = row['team']\n",
    "    link = row['link']\n",
    "\n",
    "    base_url = 'https://fbref.com'\n",
    "    team_url = team.replace(' ', '-')\n",
    "\n",
    "    # Generate the range of years based on the common year range\n",
    "    years = range(year_i, year_f + 1)\n",
    "\n",
    "    combined_data = []\n",
    "\n",
    "    # Visit each year's URL for the team\n",
    "    for year in years:\n",
    "        url = f'{base_url}/en/squads/{link}/{year}-{year + 1}/{team_url}-Stats'\n",
    "        \n",
    "        # Print the generated URLs for each team\n",
    "        print(f'Team: {team}, URLs: {url}')\n",
    "        \n",
    "        # Visit the page and create the soup object\n",
    "        browser.visit(url)\n",
    "        time.sleep(5)\n",
    "        browser.is_element_present_by_id('stats_standard_9', wait_time=5)\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Scrape the offensive table from the page\n",
    "        table = soup.find('table', {'class': 'stats_table sortable min_width now_sortable sticky_table eq1 re1 le1', \n",
    "                                    'id': 'stats_standard_9'})\n",
    "\n",
    "        # Execute JavaScript to stop further page loading\n",
    "        browser.execute_script(\"window.stop();\")\n",
    "\n",
    "        # Extract data from the table\n",
    "        data_list = []\n",
    "\n",
    "        # Iterate through the rows in the table body\n",
    "        for row in table.find('tbody').find_all('tr'):\n",
    "            # Extract the 'Date' from the <th> with scope 'row' and class 'left'\n",
    "            player_elem = row.find('th', {'scope': 'row', 'class': 'left'})\n",
    "            player = player_elem.get_text() if player_elem else None\n",
    "\n",
    "            # Other columns extracted from <td> elements\n",
    "            columns = [cell.get_text() for cell in row.find_all('td')]\n",
    "\n",
    "            # Insert 'date' into the beginning of the columns list if it exists\n",
    "            if player_elem:\n",
    "                columns.insert(0, player)\n",
    "\n",
    "            # Append the row data to the list\n",
    "            data_list.append(columns)\n",
    "\n",
    "        # Append the scraped data to combined_data\n",
    "        combined_data.extend(data_list)\n",
    "\n",
    "    # Define columns including 'Notes'\n",
    "    columns = ['Player', 'Nation', 'Pos', 'Age', 'MP', 'Starts', 'Min', '90s', 'Gls', 'Ast', 'G+A', 'G-PK', 'PK', 'PKatt',\n",
    "              'CrdY', 'CrdR', 'xG', 'npxG', 'xAG', 'npxG+xAG', 'PrgC', 'PrgP', 'PrgR', 'Gls/90', 'Ast/90', 'G+A/90', 'G-PK/90',\n",
    "              'G+A-PK/90', 'xG/90', 'xAG/90', 'xG+xAG/90', 'npxG/90', 'npxG+xAG/90', 'Matches'\n",
    "]\n",
    "\n",
    "    # Create a Pandas DataFrame\n",
    "    team_df = pd.DataFrame(combined_data, columns=columns)\n",
    "\n",
    "    # Save the data for each team to a separate CSV file\n",
    "    # Assuming 'team_df' contains the scraped and processed data for each team\n",
    "    team_df.to_csv(f'../Shooting_Stats/{team}_standard_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quit the browser session\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e69ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
